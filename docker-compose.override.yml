# docker-compose.override.yml
services:
  localstack:
    image: docker.io/localstack/localstack:latest
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3,iam,secretsmanager,lambda,sqs,sns,events,dynamodb
      - DOCKER_HOST=unix:///var/run/docker.sock
      - HOSTNAME_EXTERNAL=localstack # This helps with "Internal vs External" URL resolution when Spark tries to find the S3 buckets.
    volumes:
      - "${XDG_RUNTIME_DIR}/podman/podman.sock:/var/run/docker.sock:ro"
      - "./.localstack:/var/lib/localstack"
    networks:
      - iceberg_net

  spark-iceberg:
    image: tabulario/spark-iceberg
    container_name: spark-iceberg
    build: spark/
    networks:
      iceberg_net:
    depends_on:
      - rest
      - localstack
    volumes:
      - ./warehouse:/home/iceberg/warehouse
      - ./notebooks:/home/iceberg/notebooks/notebooks
    environment:
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_REGION=us-east-1
      - SPARK_CONF_spark_hadoop_fs_s3a_endpoint=http://localstack:4566
      - SPARK_CONF_spark_hadoop_fs_s3a_path_style_access=true
      - SPARK_CONF_spark_hadoop_fs_s3a_impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    ports:
      - 8888:8888
      - 8081:8080
      - 10000:10000
      - 10001:10001

  rest:
    image: apache/iceberg-rest-fixture
    container_name: iceberg-rest
    networks:
      iceberg_net:
    ports:
      - 8181:8181
    environment:
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://localstack:4566
      - CATALOG_S3_PATH_STYLE_ACCESS=true

networks:
  iceberg_net:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: "1500"

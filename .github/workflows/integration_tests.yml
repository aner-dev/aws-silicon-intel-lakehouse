name: Data Pipeline Integration Test

on: [push, pull_request]

jobs:
  spark-test:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
      AWS_DEFAULT_REGION: us-east-1
      AWS_ENDPOINT_URL: http://localhost:4566 # Algunos SDKs lo leen de aqu√≠

    steps:
      - uses: actions/checkout@v4

      # 1. Start LocalStack
      - name: Start LocalStack
        run: |
          docker run -d --name localstack -p 4566:4566 -p 4510-4559:4510-4559 localstack/localstack
          echo "Waiting for LocalStack to be ready..."
          timeout 60s bash -c "until curl -s localhost:4566/_localstack/health | grep '\"s3\": \"available\"'; do sleep 2; done"
      # 2. Setup Python and UV
      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install Dependencies
        run: uv sync

      # 2.5 Install Official Terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.14.3"
          terraform_wrapper: false
      # 3. Spin up Infrastructure with Terraform

      - name: Terraform Apply
        working-directory: ./infra
        run: |
          terraform init
          terraform apply -auto-approve
      # 4. Mock Data (The step you missed today)
      - name: Seed Bronze Layer
        run: |
          aws --endpoint-url=http://localhost:4566 s3 cp \
          tests/fixtures/sample_news.json s3://silicon-intel-bronze/news_data_raw.json

      # 5. Run Transformation
      - name: Run Silver Transformation
        run: uv run src/transform/silver_transform_pyspark_news_api.py
